{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lec_5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNvi2c0SNRuca4qtraai2Zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dippinchips/PyTorchZeroToAll/blob/main/lec_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3ooOW2wO9Ze",
        "outputId": "e57c9850-c173-443b-b37c-1fbec8486aab"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Loss: 121.89640808105469 \n",
            "Epoch: 1 | Loss: 54.30082702636719 \n",
            "Epoch: 2 | Loss: 24.208663940429688 \n",
            "Epoch: 3 | Loss: 10.81198501586914 \n",
            "Epoch: 4 | Loss: 4.847658157348633 \n",
            "Epoch: 5 | Loss: 2.192013740539551 \n",
            "Epoch: 6 | Loss: 1.0093094110488892 \n",
            "Epoch: 7 | Loss: 0.48232027888298035 \n",
            "Epoch: 8 | Loss: 0.24724633991718292 \n",
            "Epoch: 9 | Loss: 0.1421297937631607 \n",
            "Epoch: 10 | Loss: 0.09487415850162506 \n",
            "Epoch: 11 | Loss: 0.07338330149650574 \n",
            "Epoch: 12 | Loss: 0.06336844712495804 \n",
            "Epoch: 13 | Loss: 0.05846904218196869 \n",
            "Epoch: 14 | Loss: 0.05585303157567978 \n",
            "Epoch: 15 | Loss: 0.05425991490483284 \n",
            "Epoch: 16 | Loss: 0.053128212690353394 \n",
            "Epoch: 17 | Loss: 0.05220803618431091 \n",
            "Epoch: 18 | Loss: 0.051387883722782135 \n",
            "Epoch: 19 | Loss: 0.05061827972531319 \n",
            "Epoch: 20 | Loss: 0.04987702518701553 \n",
            "Epoch: 21 | Loss: 0.0491541288793087 \n",
            "Epoch: 22 | Loss: 0.0484449565410614 \n",
            "Epoch: 23 | Loss: 0.04774745553731918 \n",
            "Epoch: 24 | Loss: 0.04706064239144325 \n",
            "Epoch: 25 | Loss: 0.0463840514421463 \n",
            "Epoch: 26 | Loss: 0.045717403292655945 \n",
            "Epoch: 27 | Loss: 0.04506039619445801 \n",
            "Epoch: 28 | Loss: 0.04441270977258682 \n",
            "Epoch: 29 | Loss: 0.043774351477622986 \n",
            "Epoch: 30 | Loss: 0.04314534366130829 \n",
            "Epoch: 31 | Loss: 0.04252523183822632 \n",
            "Epoch: 32 | Loss: 0.041914090514183044 \n",
            "Epoch: 33 | Loss: 0.04131177067756653 \n",
            "Epoch: 34 | Loss: 0.04071801155805588 \n",
            "Epoch: 35 | Loss: 0.04013286903500557 \n",
            "Epoch: 36 | Loss: 0.03955609351396561 \n",
            "Epoch: 37 | Loss: 0.038987595587968826 \n",
            "Epoch: 38 | Loss: 0.038427263498306274 \n",
            "Epoch: 39 | Loss: 0.0378749817609787 \n",
            "Epoch: 40 | Loss: 0.03733060508966446 \n",
            "Epoch: 41 | Loss: 0.03679420053958893 \n",
            "Epoch: 42 | Loss: 0.036265380680561066 \n",
            "Epoch: 43 | Loss: 0.03574422746896744 \n",
            "Epoch: 44 | Loss: 0.03523048013448715 \n",
            "Epoch: 45 | Loss: 0.03472423553466797 \n",
            "Epoch: 46 | Loss: 0.03422516584396362 \n",
            "Epoch: 47 | Loss: 0.03373333439230919 \n",
            "Epoch: 48 | Loss: 0.03324837237596512 \n",
            "Epoch: 49 | Loss: 0.03277060016989708 \n",
            "Epoch: 50 | Loss: 0.032299600541591644 \n",
            "Epoch: 51 | Loss: 0.0318354032933712 \n",
            "Epoch: 52 | Loss: 0.0313778892159462 \n",
            "Epoch: 53 | Loss: 0.030926954001188278 \n",
            "Epoch: 54 | Loss: 0.030482536181807518 \n",
            "Epoch: 55 | Loss: 0.03004446253180504 \n",
            "Epoch: 56 | Loss: 0.029612578451633453 \n",
            "Epoch: 57 | Loss: 0.029187027364969254 \n",
            "Epoch: 58 | Loss: 0.02876758761703968 \n",
            "Epoch: 59 | Loss: 0.028354112058877945 \n",
            "Epoch: 60 | Loss: 0.02794659323990345 \n",
            "Epoch: 61 | Loss: 0.02754499390721321 \n",
            "Epoch: 62 | Loss: 0.027149111032485962 \n",
            "Epoch: 63 | Loss: 0.02675897441804409 \n",
            "Epoch: 64 | Loss: 0.026374362409114838 \n",
            "Epoch: 65 | Loss: 0.025995329022407532 \n",
            "Epoch: 66 | Loss: 0.025621794164180756 \n",
            "Epoch: 67 | Loss: 0.025253482162952423 \n",
            "Epoch: 68 | Loss: 0.024890625849366188 \n",
            "Epoch: 69 | Loss: 0.024532929062843323 \n",
            "Epoch: 70 | Loss: 0.02418031357228756 \n",
            "Epoch: 71 | Loss: 0.02383287437260151 \n",
            "Epoch: 72 | Loss: 0.023490292951464653 \n",
            "Epoch: 73 | Loss: 0.023152748122811317 \n",
            "Epoch: 74 | Loss: 0.022819936275482178 \n",
            "Epoch: 75 | Loss: 0.02249203994870186 \n",
            "Epoch: 76 | Loss: 0.022168748080730438 \n",
            "Epoch: 77 | Loss: 0.021850204095244408 \n",
            "Epoch: 78 | Loss: 0.021536175161600113 \n",
            "Epoch: 79 | Loss: 0.021226629614830017 \n",
            "Epoch: 80 | Loss: 0.020921528339385986 \n",
            "Epoch: 81 | Loss: 0.02062094956636429 \n",
            "Epoch: 82 | Loss: 0.02032454125583172 \n",
            "Epoch: 83 | Loss: 0.0200324859470129 \n",
            "Epoch: 84 | Loss: 0.019744552671909332 \n",
            "Epoch: 85 | Loss: 0.01946074888110161 \n",
            "Epoch: 86 | Loss: 0.01918111741542816 \n",
            "Epoch: 87 | Loss: 0.018905436620116234 \n",
            "Epoch: 88 | Loss: 0.01863369718194008 \n",
            "Epoch: 89 | Loss: 0.018365852534770966 \n",
            "Epoch: 90 | Loss: 0.018102029338479042 \n",
            "Epoch: 91 | Loss: 0.017841830849647522 \n",
            "Epoch: 92 | Loss: 0.01758541911840439 \n",
            "Epoch: 93 | Loss: 0.017332648858428 \n",
            "Epoch: 94 | Loss: 0.01708357036113739 \n",
            "Epoch: 95 | Loss: 0.016838103532791138 \n",
            "Epoch: 96 | Loss: 0.016596026718616486 \n",
            "Epoch: 97 | Loss: 0.016357535496354103 \n",
            "Epoch: 98 | Loss: 0.016122424975037575 \n",
            "Epoch: 99 | Loss: 0.01589074172079563 \n",
            "Epoch: 100 | Loss: 0.01566244289278984 \n",
            "Epoch: 101 | Loss: 0.015437334775924683 \n",
            "Epoch: 102 | Loss: 0.015215435065329075 \n",
            "Epoch: 103 | Loss: 0.01499675028026104 \n",
            "Epoch: 104 | Loss: 0.01478123851120472 \n",
            "Epoch: 105 | Loss: 0.014568802900612354 \n",
            "Epoch: 106 | Loss: 0.01435947883874178 \n",
            "Epoch: 107 | Loss: 0.014153088442981243 \n",
            "Epoch: 108 | Loss: 0.013949654996395111 \n",
            "Epoch: 109 | Loss: 0.013749152421951294 \n",
            "Epoch: 110 | Loss: 0.013551597483456135 \n",
            "Epoch: 111 | Loss: 0.013356849551200867 \n",
            "Epoch: 112 | Loss: 0.013164905831217766 \n",
            "Epoch: 113 | Loss: 0.012975715100765228 \n",
            "Epoch: 114 | Loss: 0.012789195403456688 \n",
            "Epoch: 115 | Loss: 0.012605404481291771 \n",
            "Epoch: 116 | Loss: 0.012424246408045292 \n",
            "Epoch: 117 | Loss: 0.012245647609233856 \n",
            "Epoch: 118 | Loss: 0.012069694697856903 \n",
            "Epoch: 119 | Loss: 0.011896221898496151 \n",
            "Epoch: 120 | Loss: 0.011725295335054398 \n",
            "Epoch: 121 | Loss: 0.011556759476661682 \n",
            "Epoch: 122 | Loss: 0.011390657164156437 \n",
            "Epoch: 123 | Loss: 0.011226944625377655 \n",
            "Epoch: 124 | Loss: 0.011065608821809292 \n",
            "Epoch: 125 | Loss: 0.01090658362954855 \n",
            "Epoch: 126 | Loss: 0.010749866254627705 \n",
            "Epoch: 127 | Loss: 0.010595325380563736 \n",
            "Epoch: 128 | Loss: 0.01044305507093668 \n",
            "Epoch: 129 | Loss: 0.010292978957295418 \n",
            "Epoch: 130 | Loss: 0.01014510728418827 \n",
            "Epoch: 131 | Loss: 0.009999229572713375 \n",
            "Epoch: 132 | Loss: 0.009855562821030617 \n",
            "Epoch: 133 | Loss: 0.009713887237012386 \n",
            "Epoch: 134 | Loss: 0.009574318304657936 \n",
            "Epoch: 135 | Loss: 0.009436730295419693 \n",
            "Epoch: 136 | Loss: 0.00930110365152359 \n",
            "Epoch: 137 | Loss: 0.009167399257421494 \n",
            "Epoch: 138 | Loss: 0.009035673923790455 \n",
            "Epoch: 139 | Loss: 0.008905785158276558 \n",
            "Epoch: 140 | Loss: 0.00877787172794342 \n",
            "Epoch: 141 | Loss: 0.008651696145534515 \n",
            "Epoch: 142 | Loss: 0.008527340367436409 \n",
            "Epoch: 143 | Loss: 0.00840480625629425 \n",
            "Epoch: 144 | Loss: 0.008283985778689384 \n",
            "Epoch: 145 | Loss: 0.00816495344042778 \n",
            "Epoch: 146 | Loss: 0.008047637529671192 \n",
            "Epoch: 147 | Loss: 0.007931932806968689 \n",
            "Epoch: 148 | Loss: 0.007817954756319523 \n",
            "Epoch: 149 | Loss: 0.007705566938966513 \n",
            "Epoch: 150 | Loss: 0.007594869937747717 \n",
            "Epoch: 151 | Loss: 0.007485708221793175 \n",
            "Epoch: 152 | Loss: 0.007378105074167252 \n",
            "Epoch: 153 | Loss: 0.007272110320627689 \n",
            "Epoch: 154 | Loss: 0.007167595438659191 \n",
            "Epoch: 155 | Loss: 0.007064569741487503 \n",
            "Epoch: 156 | Loss: 0.006963047198951244 \n",
            "Epoch: 157 | Loss: 0.006862948648631573 \n",
            "Epoch: 158 | Loss: 0.006764349061995745 \n",
            "Epoch: 159 | Loss: 0.006667103618383408 \n",
            "Epoch: 160 | Loss: 0.006571306847035885 \n",
            "Epoch: 161 | Loss: 0.006476864218711853 \n",
            "Epoch: 162 | Loss: 0.006383768282830715 \n",
            "Epoch: 163 | Loss: 0.006292038597166538 \n",
            "Epoch: 164 | Loss: 0.006201614160090685 \n",
            "Epoch: 165 | Loss: 0.006112498231232166 \n",
            "Epoch: 166 | Loss: 0.006024600006639957 \n",
            "Epoch: 167 | Loss: 0.005938065238296986 \n",
            "Epoch: 168 | Loss: 0.00585272116586566 \n",
            "Epoch: 169 | Loss: 0.00576860411092639 \n",
            "Epoch: 170 | Loss: 0.005685669835656881 \n",
            "Epoch: 171 | Loss: 0.0056039937771856785 \n",
            "Epoch: 172 | Loss: 0.0055234311148524284 \n",
            "Epoch: 173 | Loss: 0.005444052629172802 \n",
            "Epoch: 174 | Loss: 0.005365828517824411 \n",
            "Epoch: 175 | Loss: 0.005288694053888321 \n",
            "Epoch: 176 | Loss: 0.005212697200477123 \n",
            "Epoch: 177 | Loss: 0.0051378048956394196 \n",
            "Epoch: 178 | Loss: 0.005063943099230528 \n",
            "Epoch: 179 | Loss: 0.004991145338863134 \n",
            "Epoch: 180 | Loss: 0.004919450264424086 \n",
            "Epoch: 181 | Loss: 0.004848734941333532 \n",
            "Epoch: 182 | Loss: 0.004779034294188023 \n",
            "Epoch: 183 | Loss: 0.004710376728326082 \n",
            "Epoch: 184 | Loss: 0.00464269146323204 \n",
            "Epoch: 185 | Loss: 0.004575932864099741 \n",
            "Epoch: 186 | Loss: 0.004510181024670601 \n",
            "Epoch: 187 | Loss: 0.004445385187864304 \n",
            "Epoch: 188 | Loss: 0.004381474107503891 \n",
            "Epoch: 189 | Loss: 0.004318518564105034 \n",
            "Epoch: 190 | Loss: 0.004256458953022957 \n",
            "Epoch: 191 | Loss: 0.004195288754999638 \n",
            "Epoch: 192 | Loss: 0.004134965594857931 \n",
            "Epoch: 193 | Loss: 0.00407556165009737 \n",
            "Epoch: 194 | Loss: 0.00401699636131525 \n",
            "Epoch: 195 | Loss: 0.003959231544286013 \n",
            "Epoch: 196 | Loss: 0.0039023542776703835 \n",
            "Epoch: 197 | Loss: 0.003846266306936741 \n",
            "Epoch: 198 | Loss: 0.0037909788079559803 \n",
            "Epoch: 199 | Loss: 0.003736528567969799 \n",
            "Epoch: 200 | Loss: 0.0036828003358095884 \n",
            "Epoch: 201 | Loss: 0.00362986046820879 \n",
            "Epoch: 202 | Loss: 0.003577693598344922 \n",
            "Epoch: 203 | Loss: 0.0035262908786535263 \n",
            "Epoch: 204 | Loss: 0.0034755992237478495 \n",
            "Epoch: 205 | Loss: 0.003425645874813199 \n",
            "Epoch: 206 | Loss: 0.003376438980922103 \n",
            "Epoch: 207 | Loss: 0.0033279138151556253 \n",
            "Epoch: 208 | Loss: 0.0032800594344735146 \n",
            "Epoch: 209 | Loss: 0.0032329480163753033 \n",
            "Epoch: 210 | Loss: 0.0031864927150309086 \n",
            "Epoch: 211 | Loss: 0.0031406735070049763 \n",
            "Epoch: 212 | Loss: 0.0030955481342971325 \n",
            "Epoch: 213 | Loss: 0.003051060950383544 \n",
            "Epoch: 214 | Loss: 0.003007227322086692 \n",
            "Epoch: 215 | Loss: 0.002963992767035961 \n",
            "Epoch: 216 | Loss: 0.0029214012902230024 \n",
            "Epoch: 217 | Loss: 0.002879420528188348 \n",
            "Epoch: 218 | Loss: 0.002838028594851494 \n",
            "Epoch: 219 | Loss: 0.002797225257381797 \n",
            "Epoch: 220 | Loss: 0.0027570382226258516 \n",
            "Epoch: 221 | Loss: 0.002717420691624284 \n",
            "Epoch: 222 | Loss: 0.002678357530385256 \n",
            "Epoch: 223 | Loss: 0.002639884827658534 \n",
            "Epoch: 224 | Loss: 0.002601936925202608 \n",
            "Epoch: 225 | Loss: 0.0025645229034125805 \n",
            "Epoch: 226 | Loss: 0.0025276951491832733 \n",
            "Epoch: 227 | Loss: 0.0024913691449910402 \n",
            "Epoch: 228 | Loss: 0.002455542329698801 \n",
            "Epoch: 229 | Loss: 0.0024202694185078144 \n",
            "Epoch: 230 | Loss: 0.0023854675237089396 \n",
            "Epoch: 231 | Loss: 0.0023511883337050676 \n",
            "Epoch: 232 | Loss: 0.002317414851859212 \n",
            "Epoch: 233 | Loss: 0.0022841135505586863 \n",
            "Epoch: 234 | Loss: 0.0022512711584568024 \n",
            "Epoch: 235 | Loss: 0.00221892143599689 \n",
            "Epoch: 236 | Loss: 0.0021870313212275505 \n",
            "Epoch: 237 | Loss: 0.0021556071005761623 \n",
            "Epoch: 238 | Loss: 0.0021246084943413734 \n",
            "Epoch: 239 | Loss: 0.002094095107167959 \n",
            "Epoch: 240 | Loss: 0.0020640005823224783 \n",
            "Epoch: 241 | Loss: 0.0020343204960227013 \n",
            "Epoch: 242 | Loss: 0.0020050951279699802 \n",
            "Epoch: 243 | Loss: 0.0019762618467211723 \n",
            "Epoch: 244 | Loss: 0.001947876182384789 \n",
            "Epoch: 245 | Loss: 0.0019198869122192264 \n",
            "Epoch: 246 | Loss: 0.0018922872841358185 \n",
            "Epoch: 247 | Loss: 0.0018650884740054607 \n",
            "Epoch: 248 | Loss: 0.0018382810521870852 \n",
            "Epoch: 249 | Loss: 0.0018118680454790592 \n",
            "Epoch: 250 | Loss: 0.0017858280334621668 \n",
            "Epoch: 251 | Loss: 0.0017601618310436606 \n",
            "Epoch: 252 | Loss: 0.0017348728142678738 \n",
            "Epoch: 253 | Loss: 0.001709937583655119 \n",
            "Epoch: 254 | Loss: 0.0016853546258062124 \n",
            "Epoch: 255 | Loss: 0.001661146292462945 \n",
            "Epoch: 256 | Loss: 0.0016372590325772762 \n",
            "Epoch: 257 | Loss: 0.0016137291677296162 \n",
            "Epoch: 258 | Loss: 0.001590529689565301 \n",
            "Epoch: 259 | Loss: 0.0015676916809752584 \n",
            "Epoch: 260 | Loss: 0.0015451489016413689 \n",
            "Epoch: 261 | Loss: 0.001522955484688282 \n",
            "Epoch: 262 | Loss: 0.001501060207374394 \n",
            "Epoch: 263 | Loss: 0.0014795043971389532 \n",
            "Epoch: 264 | Loss: 0.0014582244912162423 \n",
            "Epoch: 265 | Loss: 0.0014372618170455098 \n",
            "Epoch: 266 | Loss: 0.0014166150940582156 \n",
            "Epoch: 267 | Loss: 0.0013962481170892715 \n",
            "Epoch: 268 | Loss: 0.00137616868596524 \n",
            "Epoch: 269 | Loss: 0.00135638692881912 \n",
            "Epoch: 270 | Loss: 0.0013369034277275205 \n",
            "Epoch: 271 | Loss: 0.0013176981592550874 \n",
            "Epoch: 272 | Loss: 0.0012987595982849598 \n",
            "Epoch: 273 | Loss: 0.001280082855373621 \n",
            "Epoch: 274 | Loss: 0.0012616984313353896 \n",
            "Epoch: 275 | Loss: 0.0012435701210051775 \n",
            "Epoch: 276 | Loss: 0.001225690939463675 \n",
            "Epoch: 277 | Loss: 0.0012080785818397999 \n",
            "Epoch: 278 | Loss: 0.0011907141888514161 \n",
            "Epoch: 279 | Loss: 0.001173592871055007 \n",
            "Epoch: 280 | Loss: 0.0011567440815269947 \n",
            "Epoch: 281 | Loss: 0.001140120904892683 \n",
            "Epoch: 282 | Loss: 0.0011237371945753694 \n",
            "Epoch: 283 | Loss: 0.0011075842194259167 \n",
            "Epoch: 284 | Loss: 0.001091663260012865 \n",
            "Epoch: 285 | Loss: 0.0010759702417999506 \n",
            "Epoch: 286 | Loss: 0.0010605044662952423 \n",
            "Epoch: 287 | Loss: 0.0010452652350068092 \n",
            "Epoch: 288 | Loss: 0.0010302446316927671 \n",
            "Epoch: 289 | Loss: 0.0010154308984056115 \n",
            "Epoch: 290 | Loss: 0.0010008512763306499 \n",
            "Epoch: 291 | Loss: 0.0009864539606496692 \n",
            "Epoch: 292 | Loss: 0.000972287671174854 \n",
            "Epoch: 293 | Loss: 0.0009583031642250717 \n",
            "Epoch: 294 | Loss: 0.0009445378673262894 \n",
            "Epoch: 295 | Loss: 0.0009309679153375328 \n",
            "Epoch: 296 | Loss: 0.0009175701416097581 \n",
            "Epoch: 297 | Loss: 0.0009043954196386039 \n",
            "Epoch: 298 | Loss: 0.0008913974743336439 \n",
            "Epoch: 299 | Loss: 0.0008785746758803725 \n",
            "Epoch: 300 | Loss: 0.0008659610757604241 \n",
            "Epoch: 301 | Loss: 0.0008535169763490558 \n",
            "Epoch: 302 | Loss: 0.000841235276311636 \n",
            "Epoch: 303 | Loss: 0.0008291531121358275 \n",
            "Epoch: 304 | Loss: 0.0008172348025254905 \n",
            "Epoch: 305 | Loss: 0.0008054903009906411 \n",
            "Epoch: 306 | Loss: 0.0007939142524264753 \n",
            "Epoch: 307 | Loss: 0.0007825099164620042 \n",
            "Epoch: 308 | Loss: 0.0007712736842222512 \n",
            "Epoch: 309 | Loss: 0.0007601876277476549 \n",
            "Epoch: 310 | Loss: 0.0007492549484595656 \n",
            "Epoch: 311 | Loss: 0.0007384946802631021 \n",
            "Epoch: 312 | Loss: 0.000727867241948843 \n",
            "Epoch: 313 | Loss: 0.000717413320671767 \n",
            "Epoch: 314 | Loss: 0.0007071169675327837 \n",
            "Epoch: 315 | Loss: 0.000696942675858736 \n",
            "Epoch: 316 | Loss: 0.0006869197823107243 \n",
            "Epoch: 317 | Loss: 0.0006770587060600519 \n",
            "Epoch: 318 | Loss: 0.0006673245807178319 \n",
            "Epoch: 319 | Loss: 0.0006577429012395442 \n",
            "Epoch: 320 | Loss: 0.0006482793833129108 \n",
            "Epoch: 321 | Loss: 0.0006389564950950444 \n",
            "Epoch: 322 | Loss: 0.0006297710933722556 \n",
            "Epoch: 323 | Loss: 0.0006207348196767271 \n",
            "Epoch: 324 | Loss: 0.0006118026794865727 \n",
            "Epoch: 325 | Loss: 0.0006030094809830189 \n",
            "Epoch: 326 | Loss: 0.0005943451542407274 \n",
            "Epoch: 327 | Loss: 0.0005858096410520375 \n",
            "Epoch: 328 | Loss: 0.000577384780626744 \n",
            "Epoch: 329 | Loss: 0.0005690917605534196 \n",
            "Epoch: 330 | Loss: 0.0005609123036265373 \n",
            "Epoch: 331 | Loss: 0.0005528454203158617 \n",
            "Epoch: 332 | Loss: 0.0005449062446132302 \n",
            "Epoch: 333 | Loss: 0.0005370729486458004 \n",
            "Epoch: 334 | Loss: 0.0005293579306453466 \n",
            "Epoch: 335 | Loss: 0.0005217372090555727 \n",
            "Epoch: 336 | Loss: 0.000514249550178647 \n",
            "Epoch: 337 | Loss: 0.000506865733768791 \n",
            "Epoch: 338 | Loss: 0.0004995756316930056 \n",
            "Epoch: 339 | Loss: 0.0004923989763483405 \n",
            "Epoch: 340 | Loss: 0.00048531772335991263 \n",
            "Epoch: 341 | Loss: 0.00047834505676291883 \n",
            "Epoch: 342 | Loss: 0.0004714796377811581 \n",
            "Epoch: 343 | Loss: 0.00046469271183013916 \n",
            "Epoch: 344 | Loss: 0.0004580096574500203 \n",
            "Epoch: 345 | Loss: 0.0004514331230893731 \n",
            "Epoch: 346 | Loss: 0.0004449482948984951 \n",
            "Epoch: 347 | Loss: 0.0004385566571727395 \n",
            "Epoch: 348 | Loss: 0.00043224633554928005 \n",
            "Epoch: 349 | Loss: 0.00042603453039191663 \n",
            "Epoch: 350 | Loss: 0.0004199141403660178 \n",
            "Epoch: 351 | Loss: 0.00041388062527403235 \n",
            "Epoch: 352 | Loss: 0.00040792481740936637 \n",
            "Epoch: 353 | Loss: 0.00040206697303801775 \n",
            "Epoch: 354 | Loss: 0.0003962955088354647 \n",
            "Epoch: 355 | Loss: 0.00039059907430782914 \n",
            "Epoch: 356 | Loss: 0.0003849849454127252 \n",
            "Epoch: 357 | Loss: 0.0003794476797338575 \n",
            "Epoch: 358 | Loss: 0.00037399103166535497 \n",
            "Epoch: 359 | Loss: 0.0003686151758302003 \n",
            "Epoch: 360 | Loss: 0.0003633215092122555 \n",
            "Epoch: 361 | Loss: 0.00035809690598398447 \n",
            "Epoch: 362 | Loss: 0.00035294509143568575 \n",
            "Epoch: 363 | Loss: 0.00034787834738381207 \n",
            "Epoch: 364 | Loss: 0.00034287836751900613 \n",
            "Epoch: 365 | Loss: 0.0003379498375579715 \n",
            "Epoch: 366 | Loss: 0.00033309415448457 \n",
            "Epoch: 367 | Loss: 0.00032831361750140786 \n",
            "Epoch: 368 | Loss: 0.000323591724736616 \n",
            "Epoch: 369 | Loss: 0.0003189413982909173 \n",
            "Epoch: 370 | Loss: 0.00031435565324500203 \n",
            "Epoch: 371 | Loss: 0.00030983483884483576 \n",
            "Epoch: 372 | Loss: 0.0003053914988413453 \n",
            "Epoch: 373 | Loss: 0.0003009973734151572 \n",
            "Epoch: 374 | Loss: 0.0002966701576951891 \n",
            "Epoch: 375 | Loss: 0.0002924120344687253 \n",
            "Epoch: 376 | Loss: 0.00028820050647482276 \n",
            "Epoch: 377 | Loss: 0.0002840626402758062 \n",
            "Epoch: 378 | Loss: 0.00027998583391308784 \n",
            "Epoch: 379 | Loss: 0.0002759635681286454 \n",
            "Epoch: 380 | Loss: 0.0002719934855122119 \n",
            "Epoch: 381 | Loss: 0.0002680826000869274 \n",
            "Epoch: 382 | Loss: 0.0002642245963215828 \n",
            "Epoch: 383 | Loss: 0.0002604321052785963 \n",
            "Epoch: 384 | Loss: 0.0002566865587141365 \n",
            "Epoch: 385 | Loss: 0.00025299686240032315 \n",
            "Epoch: 386 | Loss: 0.0002493687206879258 \n",
            "Epoch: 387 | Loss: 0.00024577867588959634 \n",
            "Epoch: 388 | Loss: 0.0002422508259769529 \n",
            "Epoch: 389 | Loss: 0.00023876468185335398 \n",
            "Epoch: 390 | Loss: 0.0002353349991608411 \n",
            "Epoch: 391 | Loss: 0.00023195314861368388 \n",
            "Epoch: 392 | Loss: 0.00022861776233185083 \n",
            "Epoch: 393 | Loss: 0.00022533097944688052 \n",
            "Epoch: 394 | Loss: 0.00022209396411199123 \n",
            "Epoch: 395 | Loss: 0.00021890102652832866 \n",
            "Epoch: 396 | Loss: 0.00021575597929768264 \n",
            "Epoch: 397 | Loss: 0.00021265828399918973 \n",
            "Epoch: 398 | Loss: 0.00020959728863090277 \n",
            "Epoch: 399 | Loss: 0.00020658942230511457 \n",
            "Epoch: 400 | Loss: 0.00020362070063129067 \n",
            "Epoch: 401 | Loss: 0.0002006932336371392 \n",
            "Epoch: 402 | Loss: 0.00019780985894612968 \n",
            "Epoch: 403 | Loss: 0.000194959546206519 \n",
            "Epoch: 404 | Loss: 0.0001921661605592817 \n",
            "Epoch: 405 | Loss: 0.0001894001616165042 \n",
            "Epoch: 406 | Loss: 0.00018667802214622498 \n",
            "Epoch: 407 | Loss: 0.00018399846157990396 \n",
            "Epoch: 408 | Loss: 0.00018134847050532699 \n",
            "Epoch: 409 | Loss: 0.00017874954210128635 \n",
            "Epoch: 410 | Loss: 0.00017617325647734106 \n",
            "Epoch: 411 | Loss: 0.00017364478844683617 \n",
            "Epoch: 412 | Loss: 0.0001711528457235545 \n",
            "Epoch: 413 | Loss: 0.0001686894684098661 \n",
            "Epoch: 414 | Loss: 0.00016626573051325977 \n",
            "Epoch: 415 | Loss: 0.00016387586947530508 \n",
            "Epoch: 416 | Loss: 0.000161519565153867 \n",
            "Epoch: 417 | Loss: 0.00015919795259833336 \n",
            "Epoch: 418 | Loss: 0.00015690915461163968 \n",
            "Epoch: 419 | Loss: 0.00015466008335351944 \n",
            "Epoch: 420 | Loss: 0.00015243585221469402 \n",
            "Epoch: 421 | Loss: 0.00015024413005448878 \n",
            "Epoch: 422 | Loss: 0.00014808453852310777 \n",
            "Epoch: 423 | Loss: 0.00014595533139072359 \n",
            "Epoch: 424 | Loss: 0.00014385898248292506 \n",
            "Epoch: 425 | Loss: 0.00014179095160216093 \n",
            "Epoch: 426 | Loss: 0.00013975509500596672 \n",
            "Epoch: 427 | Loss: 0.00013774486433248967 \n",
            "Epoch: 428 | Loss: 0.0001357620640192181 \n",
            "Epoch: 429 | Loss: 0.0001338158326689154 \n",
            "Epoch: 430 | Loss: 0.00013188767479732633 \n",
            "Epoch: 431 | Loss: 0.0001299927243962884 \n",
            "Epoch: 432 | Loss: 0.0001281266158912331 \n",
            "Epoch: 433 | Loss: 0.00012628969852812588 \n",
            "Epoch: 434 | Loss: 0.00012447511835489422 \n",
            "Epoch: 435 | Loss: 0.00012268268619664013 \n",
            "Epoch: 436 | Loss: 0.00012091732060071081 \n",
            "Epoch: 437 | Loss: 0.00011918378004338592 \n",
            "Epoch: 438 | Loss: 0.00011746843665605411 \n",
            "Epoch: 439 | Loss: 0.00011577929399209097 \n",
            "Epoch: 440 | Loss: 0.00011411791638238356 \n",
            "Epoch: 441 | Loss: 0.00011247531801927835 \n",
            "Epoch: 442 | Loss: 0.00011085992446169257 \n",
            "Epoch: 443 | Loss: 0.00010926291724899784 \n",
            "Epoch: 444 | Loss: 0.00010769257642095909 \n",
            "Epoch: 445 | Loss: 0.00010614858183544129 \n",
            "Epoch: 446 | Loss: 0.00010462226782692596 \n",
            "Epoch: 447 | Loss: 0.00010312177619198337 \n",
            "Epoch: 448 | Loss: 0.0001016403257381171 \n",
            "Epoch: 449 | Loss: 0.00010017774911830202 \n",
            "Epoch: 450 | Loss: 9.873443195829168e-05 \n",
            "Epoch: 451 | Loss: 9.731420141179115e-05 \n",
            "Epoch: 452 | Loss: 9.591793059371412e-05 \n",
            "Epoch: 453 | Loss: 9.45402352954261e-05 \n",
            "Epoch: 454 | Loss: 9.31809117901139e-05 \n",
            "Epoch: 455 | Loss: 9.184314694721252e-05 \n",
            "Epoch: 456 | Loss: 9.052445238921791e-05 \n",
            "Epoch: 457 | Loss: 8.922188862925395e-05 \n",
            "Epoch: 458 | Loss: 8.793803135631606e-05 \n",
            "Epoch: 459 | Loss: 8.667971997056156e-05 \n",
            "Epoch: 460 | Loss: 8.543369767721742e-05 \n",
            "Epoch: 461 | Loss: 8.420629455940798e-05 \n",
            "Epoch: 462 | Loss: 8.299361070385203e-05 \n",
            "Epoch: 463 | Loss: 8.179865835700184e-05 \n",
            "Epoch: 464 | Loss: 8.062437700573355e-05 \n",
            "Epoch: 465 | Loss: 7.946584082674235e-05 \n",
            "Epoch: 466 | Loss: 7.832032861188054e-05 \n",
            "Epoch: 467 | Loss: 7.719692075625062e-05 \n",
            "Epoch: 468 | Loss: 7.608567830175161e-05 \n",
            "Epoch: 469 | Loss: 7.499458297388628e-05 \n",
            "Epoch: 470 | Loss: 7.392236148007214e-05 \n",
            "Epoch: 471 | Loss: 7.28588638594374e-05 \n",
            "Epoch: 472 | Loss: 7.18089722795412e-05 \n",
            "Epoch: 473 | Loss: 7.077355985529721e-05 \n",
            "Epoch: 474 | Loss: 6.975779979256913e-05 \n",
            "Epoch: 475 | Loss: 6.875661347294226e-05 \n",
            "Epoch: 476 | Loss: 6.777272210456431e-05 \n",
            "Epoch: 477 | Loss: 6.679354555672035e-05 \n",
            "Epoch: 478 | Loss: 6.583799404324964e-05 \n",
            "Epoch: 479 | Loss: 6.488886720035225e-05 \n",
            "Epoch: 480 | Loss: 6.395825039362535e-05 \n",
            "Epoch: 481 | Loss: 6.303713598754257e-05 \n",
            "Epoch: 482 | Loss: 6.213323649717495e-05 \n",
            "Epoch: 483 | Loss: 6.123905041022226e-05 \n",
            "Epoch: 484 | Loss: 6.035767000867054e-05 \n",
            "Epoch: 485 | Loss: 5.94907469348982e-05 \n",
            "Epoch: 486 | Loss: 5.8634996094042435e-05 \n",
            "Epoch: 487 | Loss: 5.77964965486899e-05 \n",
            "Epoch: 488 | Loss: 5.6960529036587104e-05 \n",
            "Epoch: 489 | Loss: 5.614371184492484e-05 \n",
            "Epoch: 490 | Loss: 5.533885268960148e-05 \n",
            "Epoch: 491 | Loss: 5.4540672863367945e-05 \n",
            "Epoch: 492 | Loss: 5.375680484576151e-05 \n",
            "Epoch: 493 | Loss: 5.29883400304243e-05 \n",
            "Epoch: 494 | Loss: 5.2223742386559024e-05 \n",
            "Epoch: 495 | Loss: 5.147595948074013e-05 \n",
            "Epoch: 496 | Loss: 5.073356442153454e-05 \n",
            "Epoch: 497 | Loss: 5.0005204684566706e-05 \n",
            "Epoch: 498 | Loss: 4.9288628360955045e-05 \n",
            "Epoch: 499 | Loss: 4.8577639972791076e-05 \n",
            "Prediction (after training) 4 7.991988182067871\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}